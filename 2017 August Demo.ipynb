{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017 August Duplicate Bug Detection\n",
    "\n",
    "[**Find more on wiki**](https://wiki.nvidia.com/itappdev/index.php/Duplicate_Detection)\n",
    "\n",
    "[**Demo Link**](http://qlan-vm-1.client.nvidia.com:8080/)\n",
    "\n",
    "\n",
    "## Walk through of the Algorithm \n",
    "<img src=\"imgsrc/Diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning - SenteceParser Python 3\n",
    "Available on Perforce and [Github](https://github.com/lanking520/NVBugsLib)\n",
    "### Core Feature\n",
    "- NLTK: remove stopwords and do stemming\n",
    "- BeautifulSoup: Remove Html Tags\n",
    "- General Regex: clean up white spaces and other symbols\n",
    "\n",
    "### Other Functions:\n",
    "- NVBugs Specific Cleaner for Synopsis, Description and Comments\n",
    "- Counting Vectorizer embedded\n",
    "- Auto-merge Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile(self, filepath, filetype, encod ='ISO-8859-1', header =None):\n",
    "    logger.info('Start reading File')\n",
    "    if not os.path.isfile(filepath):\n",
    "        logger.error(\"File Not Exist!\")\n",
    "        sys.exit()\n",
    "    if filetype == 'csv':\n",
    "        df = pd.read_csv(filepath, encoding=encod, header =header)\n",
    "    elif filetype == 'json':\n",
    "        df = pd.read_json(filepath, encoding=encod, lines=True)\n",
    "    elif filetype == 'xlsx':\n",
    "        df = pd.read_excel(filepath, encoding=encod, header =header)\n",
    "    else:\n",
    "        logger.error(\"Extension Type not Accepted!\")\n",
    "        sys.exit()\n",
    "\n",
    "def processtext(self, column, removeSymbol = True, remove_stopwords=False, stemming=False):\n",
    "    logger.info(\"Start Data Cleaning...\")\n",
    "    self.data[column] = self.data[column].str.replace(r'[\\n\\r\\t]+', ' ')\n",
    "    # Remove URLs\n",
    "    self.data[column] = self.data[column].str.replace(self.regex_str[3],' ')\n",
    "    tempcol = self.data[column].values.tolist()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # This part takes a lot of times\n",
    "    printProgressBar(0, len(tempcol), prefix='Progress:', suffix='Complete', length=50)\n",
    "    for i in range(len(tempcol)):\n",
    "        row = BeautifulSoup(tempcol[i],'html.parser').get_text()\n",
    "        if removeSymbol:\n",
    "            row = re.sub('[^a-zA-Z0-9]', ' ', row)\n",
    "        words = row.split()\n",
    "        if remove_stopwords:\n",
    "            words = [w for w in words if not w in stops and not w.replace('.', '', 1).isdigit()]\n",
    "        row = ' '.join(words)\n",
    "        tempcol[i] = row.lower()\n",
    "        printProgressBar(i+1, len(tempcol), prefix='Progress:', suffix='Complete', length=50)\n",
    "    print(\"\\n\")\n",
    "    return tempcol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process by each line or Process by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dog home'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from SentenceParserPython3 import SentenceParser as SP\n",
    "test = SP(20)\n",
    "sample_text = \"I @#$@have a @#$@#$@#%dog @#%@$^#$()_+%at home\"\n",
    "test.processline(sample_text, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embedding\n",
    "\n",
    "### 2.1 TF-IDF\n",
    "\n",
    "**Term Frequency** denoted by tf, is the number of occurrencesof a term t in the document D.\n",
    "\n",
    "**Inverse Document Frequency** of a term t, denoted by idf is log(N/df), where N is the total number of documents in thespace. So, it reduces the weight when a term occurs manytimes in a document, or in other words a word with rareoccurrences has more weight.\n",
    "\n",
    "TF-IDF = Term Frequency * Inverse Document Frequency<br>\n",
    "Inverse Document Frequency = log(N/df)\n",
    "\n",
    "** Vocabulary size: 10000-100000 is the range used in this project **\n",
    "\n",
    "Note: TF-IDF will brings Sparse Matrix back to reduce the memory use. Sparse Matrix is supported by K-Means. Sometimes we need to tranform it into dense when we actually use it to do the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text \timport TfidfVectorizer\n",
    "\n",
    "def TFIDF(text, size):\n",
    "\tprint(\"Using TFIDF Doing data cleaning...\")\n",
    "\tvectorizer = TfidfVectorizer(stop_words='english', analyzer='word', strip_accents='unicode', max_features=size)\n",
    "\tX = vectorizer.fit_transform(text)\n",
    "\treturn vectorizer, X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Translate one\n",
    "REF: BugID 200235622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "vectorizer = joblib.load('model/MSD2016NowTFIDF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'GFE Share Telemetry item for OSC Hotkey Toggle'\n",
    "result = vectorizer.transform([sample])\n",
    "result.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Other Word Vectorization Tool\n",
    "- Hashing Vectorization\n",
    "- Word2Vec\n",
    "- Infersent (Facebook)\n",
    "- Skip-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models \t\timport word2vec\n",
    "def W2V(text, size):\n",
    "\tsentences = []\n",
    "\tfor idx in range(len(text)):\n",
    "\t\tsentences.append(text[idx].split())\n",
    "\tnum_features = size    \n",
    "\tmin_word_count = 20  \n",
    "\tnum_workers = 4       \n",
    "\tcontext = 10          \n",
    "\tdownsampling = 1e-3  \n",
    "\n",
    "\tmodel_name = \"./model/w2vec.model\"\n",
    "\n",
    "\tmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "\t\t\tsize=num_features, min_count = min_word_count, \\\n",
    "\t\t\twindow = context, sample = downsampling)\n",
    "\tmodel.init_sims(replace=True)\n",
    "\treturn model\n",
    "\n",
    "def Word2VecEmbed(text, model, num_features):\n",
    "\tworddict = {}\n",
    "\tfor key in model.wv.vocab.keys():\n",
    "\t\tworddict[key] = model.wv.word_vec(key)\n",
    "\tX = []\n",
    "\tfor idx in range(len(text)):\n",
    "\t\twords = text[idx].split()\n",
    "\t\tcounter = 0\n",
    "\t\ttemprow = np.zeros(num_features)\n",
    "\t\tfor word in words:\n",
    "\t\t\tif word in worddict:\n",
    "\t\t\t\tcounter += 1\n",
    "\t\t\t\ttemprow += worddict[word]\n",
    "\t\tif counter != 0:\n",
    "\t\t\tX.append(temprow/counter)\n",
    "\t\telse:\n",
    "\t\t\tX.append(temprow)\n",
    "\tX = np.array(X)\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear PCA\n",
    "\n",
    "**Principal component analysis (PCA)** is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components (or sometimes, principal modes of variation). The number of principal components is less than or equal to the smaller of the number of original variables or the number of observations. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "### TruncatedSVD\n",
    "Dimensionality reduction using truncated SVD (aka LSA).\n",
    "\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with scipy.sparse matrices efficiently.\n",
    "\n",
    "### Dimension Reduction\n",
    "\n",
    "In our model, we reduce the dimension from 100000 to 6000 and keep **77%** of Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition \t\t\t\timport TruncatedSVD\n",
    "from sklearn.pipeline \t\t\t\t\timport make_pipeline\n",
    "from sklearn.preprocessing \t\t\t\timport Normalizer\n",
    "def DRN(X, DRN_size):\n",
    "\tprint(\"Performing dimensionality reduction using LSA\")\n",
    "\tsvd = TruncatedSVD(DRN_size)\n",
    "\tnormalizer = Normalizer(copy=False)\n",
    "\tlsa = make_pipeline(svd, normalizer)\n",
    "\tX = lsa.fit_transform(X)\n",
    "\texplained_variance = svd.explained_variance_ratio_.sum()\n",
    "\tprint(\"Explained variance of the SVD step: {}%\".format( int(explained_variance * 100)))\n",
    "\treturn svd, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering\n",
    "\n",
    "**clustering** is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics.\n",
    "\n",
    "### 4.1 KMeans Clustering\n",
    "The current Algorithm we are using is the General KM without Mini-Batches. Mini-Batches are not working as well as the normal K-Means in our dataset.\n",
    "\n",
    "### 4.2 \"Yinyang\" K-means and K-nn using NVIDIA CUDA\n",
    "\n",
    "K-means implementation is based on [\"Yinyang K-Means: A Drop-In Replacement\n",
    "of the Classic K-Means with Consistent Speedup\"](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ding15.pdf).\n",
    "While it introduces some overhead and many conditional clauses\n",
    "which are bad for CUDA, it still shows 1.6-2x speedup against the Lloyd\n",
    "algorithm. K-nearest neighbors employ the same triangle inequality idea and\n",
    "require precalculated centroids and cluster assignments, similar to the flattened\n",
    "ball tree.\n",
    "\n",
    "| Benchmarks | sklearn KMeans | KMeansRex | KMeansRex OpenMP | Serban | kmcuda | kmcuda 2 GPUs |\n",
    "|---------------------------|----------------|-----------|------------------|--------|--------|---------------|\n",
    "| speed                     | 1x             | 4.5x      | 8.2x             | 15.5x  | 17.8x  | 29.8x         |\n",
    "| memory                    | 1x             | 2x        | 2x               | 0.6x   | 0.6x   | 0.6x          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster \t\t\t\t\timport KMeans\n",
    "def kmtrain(X, num_clusters):\n",
    "\tkm = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1, verbose=1)\n",
    "\tprint(\"Clustering sparse data with %s\" % km)\n",
    "\tkm.fit(X)\n",
    "\treturn km\n",
    "\n",
    "from libKMCUDA import kmeans_cuda\n",
    "\n",
    "def cudatrain(X, num_clusters):\n",
    "\tcentroids, assignments = kmeans_cuda(X, num_clusters, verbosity=1, yinyang_t=0, seed=3)\n",
    "\treturn centroids, assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Assignment Match the Actual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = 0.0\n",
    "assignment = []\n",
    "printProgressBar(0, X.shape[0], prefix='Progress:', suffix='Complete', length=50)\n",
    "for idx, item in enumerate(X):\n",
    "    center = np.squeeze(np.sum(np.square(item - centroid), axis =1)).argsort()[0]\n",
    "    if assign[idx] == center:\n",
    "        correct +=1.0\n",
    "    assignment.append(center)\n",
    "    printProgressBar(idx, X.shape[0], prefix='Progress:', suffix='Complete'+' Acc:' + str(correct/(idx+1)), length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the Distribution based on the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = np.bincount(assignment)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Duplicate bug set to remove non-existed duplicated bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verifier = pd.read_csv('DuplicateBugs.csv',header=None)\n",
    "verifier = verifier.as_matrix()\n",
    "available = []\n",
    "printProgressBar(0, verifier.shape[0], prefix='Progress:', suffix='Complete', length=50)\n",
    "for idx, row in enumerate(verifier):\n",
    "    if not np.isnan(row).any():\n",
    "        leftcomp = df.loc[df[\"BugId\"]==int(row[0])]\n",
    "        rightcomp = df.loc[df[\"BugId\"]==int(row[1])]\n",
    "        if (not leftcomp.empty) and (not rightcomp.empty):\n",
    "            available.append([leftcomp.index[0], rightcomp.index[0]])\n",
    "    printProgressBar(idx, verifier.shape[0], prefix='Progress:', suffix='Complete', length=50)\n",
    "temp = np.array(available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Duplicated Bug set are inside of the top 3 cluster and top 5 recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctrow = 0\n",
    "correctdist = []\n",
    "vectorizer = joblib.load(root+divname+'TFIDF.pkl')\n",
    "X = vectorizer.transform(text)\n",
    "printProgressBar(0, temp.shape[0], prefix='Progress:', suffix='Complete', length=50)\n",
    "for idx, row in enumerate(temp):\n",
    "    clusterset = np.squeeze(np.sum(np.square(real_center - X[row[0]].toarray()),axis=1)).argsort()[0:3]\n",
    "    dist = []\n",
    "    for cluster in clusterset:\n",
    "        dataset = wholeX[np.array((df[\"cluster\"] == cluster).tolist())]\n",
    "        for datarow in dataset:\n",
    "            dist.append(np.sum(np.square(datarow.toarray() - wholeX[row[0]].toarray())))\n",
    "            \n",
    "    dist = np.array(dist)\n",
    "    smalldist = np.sum(np.square(wholeX[row[1]].toarray() - wholeX[row[0]].toarray()))\n",
    "    sorteddist = np.sort(dist)\n",
    "    if sorteddist.shape[0] <= 5 or smalldist <= sorteddist[5]:\n",
    "        correctrow += 1\n",
    "        correctdist.append(smalldist)\n",
    "    printProgressBar(idx, temp.shape[0], prefix='Progress:', suffix='Complete', length=50)\n",
    "    \n",
    "print(\"Accuracy: \"+ str(1.0*correctrow/temp.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bugidgetter(df, cluster, loc):\n",
    "    bigset = df.loc[df['cluster'] == cluster]\n",
    "    return bigset.iloc[[loc],:][\"BugId\"].tolist()[0]\n",
    "\n",
    "def bugindata(df, bugid):\n",
    "    return not df.loc[df[\"BugId\"]==int(bugid)].empty\n",
    "\n",
    "def predict(text, topkclusters, topktopics):\n",
    "    bugiddist = []\n",
    "    row = vectorizer.transform([text])\n",
    "    clusterset = np.squeeze(np.sum(np.square(real_center - row.toarray()),axis=1)).argsort()[0:topkclusters]\n",
    "    dist = []\n",
    "    print(clusterset)\n",
    "    for cluster in clusterset:\n",
    "        dataset = X[np.array((df[\"cluster\"] == cluster).tolist())]\n",
    "        for idx, datarow in enumerate(dataset):\n",
    "            dist.append([np.sum(np.square(datarow.toarray() - row.toarray())), cluster, idx])\n",
    "            \n",
    "    dist = np.array(dist)\n",
    "    topk = dist[dist[:,0].argsort()][0:topktopics]\n",
    "    # print(topk)\n",
    "    for idx, row in enumerate(topk):\n",
    "        bugiddist.append({'BugId':bugidgetter(df, row[1],row[2]), 'Distance': row[0]})\n",
    "    return bugiddist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
